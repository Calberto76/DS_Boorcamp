{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c636b70-2c89-4bb1-88c4-36fdb6d1e0e9",
   "metadata": {},
   "source": [
    "# Ejercicio integrador\n",
    "\n",
    "El objetivo del siguiente ejercicio es aplicar los conocimientos obtenidos sobre los algoritmos más utilizados en aprendizaje supervisado.\n",
    "\n",
    "Para ello se les brinda un dataset al cual deberán realizar las transformaciones necesarias, definir la variable objetivo, tipo de problema (Regresión/Clasificación) y entrenar un modelo en base al problema.\n",
    "\n",
    "**Dataset**: `imdb_dataset.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f4483-d2b3-4a5e-82df-b97ba4f023e7",
   "metadata": {},
   "source": [
    "## Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15957f5-365e-4939-aeed-2403e3380c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "\n",
    "\n",
    "#TODO sklearn libraries - Completar con el/los modelos predictivos\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#TODO modelos para comparar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c25dc-896a-4876-aabd-ed66331b76cb",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9660757-115c-450a-9052-d95203d76488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('') #TODO\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56260c-2084-4a7c-aae4-28a12f9549ff",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Para este ejercicio nos limitamos a tomar solo un subset de `10000` instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ea735-3a74-4e7f-9824-30e37ce7aa94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_positive = df[df['sentiment']=='positive'][:9000]\n",
    "df_negative = df[df['sentiment']=='negative'][:1000]\n",
    "\n",
    "df_review_imb = pd.concat([df_positive,df_negative ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b5165-e840-4452-9c70-a1e2b2d20d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('deep')\n",
    "\n",
    "plt.figure(figsize=(8,4), tight_layout=True)\n",
    "plt.bar(x=['Positive', 'Negative'],\n",
    "        height=df_review_imb.value_counts(['sentiment']),\n",
    "        color=colors[:2])\n",
    "plt.title('Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9af32-16f1-4e47-92fa-a0488ed710d4",
   "metadata": {},
   "source": [
    "## Preparación y análisis de datos (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89d6bc-d50a-4ed0-9299-4e35327ba522",
   "metadata": {},
   "source": [
    "## Resample data\n",
    "\n",
    "El desequilibrio de datos es un gran problema para las tareas de clasificación. En Python, existe una biblioteca que permite el uso de muchos algoritmos para manejar este estado desequilibrado de los datos y sus daños.\n",
    "\n",
    "imbalanced-learn es un paquete de Python que ofrece varias técnicas de remuestreo comúnmente utilizadas en conjuntos de datos que muestran un fuerte desequilibrio entre clases. Es compatible con scikit-learn y forma parte de proyectos `scikit-learn`.\n",
    "\n",
    "Para volver a muestrear nuestros datos utilizamos la biblioteca `imblearn`. Puede submuestrear reseñas positivas o sobremuestrear reseñas negativas (debe elegir según los datos con los que está trabajando). En este caso, usaremos `RandomUnderSampler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587c101-e727-4c88-a3c0-baad2b1b04b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state= 0)\n",
    "df_review_bal,df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],\n",
    "                                                          df_review_imb['sentiment'])\n",
    "\n",
    "\n",
    "df_review_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ff87b-5573-4bf9-b6d3-d18dbce175ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_review_imb.value_counts('sentiment'))\n",
    "print(df_review_bal.value_counts('sentiment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340eaa4-ce30-4d51-8192-e13bda4a6ad6",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "La tokenización es una forma de separar un fragmento de texto en unidades más pequeñas llamadas tokens. Aquí, los tokens pueden ser palabras, caracteres o subpalabras. Por lo tanto, la tokenización se puede clasificar en términos generales en 3 tipos: tokenización de palabras, caracteres y subpalabras (caracteres de n-gramas).\n",
    "\n",
    "Por ejemplo, considere la frase: \"Nunca te rindas\".\n",
    "\n",
    "La forma más común de formar tokens se basa en el espacio. Asumiendo el espacio como delimitador, la tokenización de la oración da como resultado 3 tokens: Nunca te rindas. Como cada token es una palabra, se convierte en un ejemplo de tokenización de Word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4423da5-39b1-43b5-b28d-fb659a014e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90af79-42b8-4de9-a3bd-a917fce3d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones para limpieza de datos\n",
    "\n",
    "def clean_data(text):          #using the re library\n",
    "\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(\"^a-zA-z0-9\\s\",\"\", text)\n",
    "    text = re.sub(r\"<br>\", \" \", text)\n",
    "    text = re.sub(r\"([-?.!,/\\\"])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,']\", \"\", text)\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = text.rstrip().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a635a-687d-4e16-8a3a-d706d783ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a87ca5-706c-4773-8f5a-10e7f54e9b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply clean data\n",
    "df_review_bal['review'] = df_review_bal.review.apply(clean_data)\n",
    "\n",
    "#Apply function on review column\n",
    "df_review_bal['review']=df_review_bal['review'].apply(denoise_text)\n",
    "\n",
    "df_review_bal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc36c2-5234-441a-ae1d-0d51869adc62",
   "metadata": {},
   "source": [
    "## Stemming text\n",
    "\n",
    "La derivación es una técnica que se utiliza para reducir una palabra flexionada hasta la raíz de la palabra. Por ejemplo, las palabras `programación`, `programador` y `programas` se pueden reducir a la raíz común de la palabra `programa`. En otras palabras, `programa` se puede utilizar como sinónimo de las tres palabras de inflexión anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46c77d-78fb-4e51-a3f6-b5c375022814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01aa008-bcf3-4559-8d18-200017a3f171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply function on review column\n",
    "df_review_bal['review']=df_review_bal['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1431a7-941d-4c3d-81d2-26b95d510f3c",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "`Stopwords` una palabra de parada de uso común (como “el”, “a”, “una”, “en”) que un motor de búsqueda ha sido programado para ignorar, tanto al indexar entradas para la búsqueda como al recuperarlas. como resultado de una consulta de búsqueda.\n",
    "\n",
    "No queremos que estas palabras ocupen espacio en nuestra base de datos ni que consuman un tiempo de procesamiento valioso. Para ello, podemos eliminarlas fácilmente, almacenando una lista de palabras que consideres vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f573f-fbe2-44d0-9736-15dfd8dde07c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e52bb4-ad70-4b47-9b34-6eca5833098f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply function on review column\n",
    "df_review_bal['review']=df_review_bal['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b8f7a-66ea-4bfb-a95f-5c50d91d295d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21540ace-cb24-418d-9b31-22012307e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b055a-72f9-4763-bc78-0f0573f474d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebe98e-81c3-4ed2-b60d-a9dc068bee48",
   "metadata": {},
   "source": [
    "## Text Representation (Bag of words)\n",
    "\n",
    "Los clasificadores y algoritmos de aprendizaje esperan vectores de características numéricas en lugar de documentos de texto sin formato. Es por eso que necesitamos convertir el texto de reseñas de películas en vectores numéricos.\n",
    "\n",
    "usaremos bolsa de palabras (BOW) ya que nos importa la frecuencia de las palabras en las revisiones de texto; sin embargo, el orden de las palabras es irrelevante. Dos formas comunes de representar una bolsa de palabras son CountVectorizer y Term Frequency, Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "Queremos identificar palabras únicas/representativas para reseñas positivas y negativas, por lo que elegiremos TF-IDF. Para convertir datos de texto en vectores numéricos con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf8fdf-97af-4ac3-a969-cbce50a37448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "train_x_vector = tfidf.fit_transform(X_train)\n",
    "\n",
    "# also fit the test_x_vector\n",
    "test_x_vector = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139337b-6068-470c-a74d-c209345e061d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entrenamiento modelo definitivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf80e20-dd02-4ee9-ba7f-40255d35a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2feed-529a-4cf0-80df-cfef1858d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testeamos el modelo\n",
    "print(# TODO.predict(tfidf.transform(['A good movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['An excellent movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['I did not like this movie at all I gave this movie away'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a500931-3f84-46b1-90bd-1ddc276ed4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6eb37-b3fe-40fc-9632-ed6ba9e3bf06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testeamos el modelo\n",
    "print(# TODO.predict(tfidf.transform(['A good movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['An excellent movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['I did not like this movie at all I gave this movie away'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002c72e-62e0-4e84-871b-75588f4d5dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6856a7-44e5-4a7b-a3a4-8a399d071bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testeamos el modelo\n",
    "print(# TODO.predict(tfidf.transform(['A good movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['An excellent movie'])))\n",
    "print(# TODO.predict(tfidf.transform(['I did not like this movie at all I gave this movie away'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e561e-974e-4d5a-a054-19e71122d1e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluación modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc231f73-0b62-42ac-b326-6c09369ec887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(# TODO.score(test_x_vector, y_test))\n",
    "print(# TODO.score(test_x_vector, y_test))\n",
    "print(# TODO.score(test_x_vector, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b41e3b-b8dd-4114-b2eb-a24297b610c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el mejor modelo y obtenemos sus métricas\n",
    "\n",
    "y_pred_test = # TODO.predict(test_x_vector)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16023b0d-fa11-457c-b803-aada6d6516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Evaluación del modelo sobre el conjunto de test\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
