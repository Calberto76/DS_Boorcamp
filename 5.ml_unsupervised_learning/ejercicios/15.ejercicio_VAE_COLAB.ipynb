{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prTKL3d2kGZE"
   },
   "source": [
    "# Variational Autoencoders en Anime Faces\n",
    "\n",
    "Para este ejercicio, entrenará un codificador automático variacional (VAE) utilizando el [conjunto de datos de caras de anime de MckInsey666](https://github.com/bchao1/Anime-Face-Dataset).\n",
    "\n",
    "Entrenarás el modelo utilizando las técnicas discutidas en clase. Al final, debe guardar su modelo y descargarlo de Colab para poder enviarlo al autocalificador para su calificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Qxq9uZAk3Lh"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3057,
     "status": "ok",
     "timestamp": 1614870283979,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "MooRFGEeI1zb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import random\n",
    "from IPython import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL9rq-0uk7nS"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1614870306502,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "pjhN6GgfmUfx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "np.random.seed(51)\n",
    "\n",
    "# parameters for building the model and training\n",
    "BATCH_SIZE=2000\n",
    "LATENT_DIM=512\n",
    "IMAGE_SIZE=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXTdjxmolDBo"
   },
   "source": [
    "## Download the Dataset\n",
    "\n",
    "Descargar el dataset en un directorio local o utilizar `Google Drive`. En el caso de usar Google Drive, subir el archivo a un direterminado y luego copiar el `path` en las variables `os.mkdir`, `data_file_name`, `downloa_dir`, y `get_dataset_slice_paths`\n",
    "\n",
    "[Link Descarga Dataset](https://drive.google.com/file/d/1YjP9-APc-LzEYbpu4egkBER0dgIQkSfc/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16191,
     "status": "ok",
     "timestamp": 1614870323912,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "qxKW6Q88KHcL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the data directory\n",
    "try:\n",
    "  os.mkdir('') #TODO\n",
    "except OSError:\n",
    "  pass\n",
    "\n",
    "data_file_name = \"\" #TODO\n",
    "download_dir = ''#TODO\n",
    "\n",
    "# extract the zip file\n",
    "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
    "zip_ref.extractall(download_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD6WCIlclWaA"
   },
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbaVpD18ggOX"
   },
   "source": [
    "Luego, prepararemos los datos para entrenamiento y validación. Debajo encontrarás algunas funciones útiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1614870332325,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "NTlx97U_JDPB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Preparation Utilities\n",
    "\n",
    "def get_dataset_slice_paths(image_dir):\n",
    "  '''returns a list of paths to the image files'''\n",
    "  image_file_list = os.listdir(image_dir)\n",
    "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "\n",
    "  return image_paths\n",
    "\n",
    "\n",
    "def map_image(image_filename):\n",
    "  '''preprocesses the images'''\n",
    "  img_raw = tf.io.read_file(image_filename)\n",
    "  image = tf.image.decode_jpeg(img_raw)\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "  image = image / 255.0  \n",
    "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uFon6vdhMhi"
   },
   "source": [
    "Utilizaremos dichas funciones para armar el set de entrenamiento y el de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6672,
     "status": "ok",
     "timestamp": 1614870340603,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "iGoCJ6DPJHL8",
    "outputId": "f4846729-adc8-4cbd-8cb4-b38d6c9f0613",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the list containing the image paths\n",
    "paths = get_dataset_slice_paths(\"\") #TODO\n",
    "\n",
    "# shuffle the paths\n",
    "random.shuffle(paths)\n",
    "\n",
    "# split the paths list into to training (80%) and validation sets(20%).\n",
    "paths_len = len(paths)\n",
    "train_paths_len = int(paths_len * ) #TODO - Completar el porcentaje de entrenamiento\n",
    "\n",
    "train_paths = paths[:train_paths_len]\n",
    "val_paths = paths[train_paths_len:]\n",
    "\n",
    "# load the training image paths into tensors, create batches and shuffle\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
    "training_dataset = training_dataset.map(map_image)\n",
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# load the validation image paths into tensors and create batches\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
    "validation_dataset = validation_dataset.map(map_image)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(f'number of batches in the training set: {len(training_dataset)}')\n",
    "print(f'number of batches in the validation set: {len(validation_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ZRga9vlonx"
   },
   "source": [
    "## Display Utilities\n",
    "\n",
    "Aquí tienes algunas funciones que te ayudarán a visualizar imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1614870366851,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "jC1cpLViJLIu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_faces(dataset, size=9):\n",
    "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
    "  dataset = dataset.unbatch().take(size)\n",
    "  n_cols = 3\n",
    "  n_rows = size//n_cols + 1\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  i = 0\n",
    "  for image in dataset:\n",
    "    i += 1\n",
    "    disp_img = np.reshape(image, ()) #TODO - (Height, Width, Channel) \n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(disp_img)\n",
    "\n",
    "\n",
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Displays a row of images.'''\n",
    "  for idx, image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    image = np.reshape(image, shape)\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_predicted):\n",
    "  '''Displays input and predicted images.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2brROh6qLJbs"
   },
   "source": [
    "Veamos algunas imágenes del dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 2989,
     "status": "ok",
     "timestamp": 1614870371927,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "5eZsrZtqJOzv",
    "outputId": "9ca3a9cb-c1ad-4bfd-81fb-c707c8aca1fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_faces(validation_dataset, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSBtdCVim9aC"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQvzWaNqLrB1"
   },
   "source": [
    "Construirás tu VAE en las siguientes secciones. Recuerda que esto seguirá la arquitectura codificador-decodificador y se puede resumir en la siguiente figura.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHNxIUUS9ng9"
   },
   "source": [
    "### Sampling Class\n",
    "\n",
    "Comenzará con la capa personalizada para proporcionar la entrada de ruido gaussiano junto con la media (mu) y la desviación estándar (sigma) de la salida del codificador. Recuerde la ecuación para combinar estos:\n",
    "\n",
    "$$z = \\mu + e^{0.5\\sigma} * \\epsilon  $$\n",
    "\n",
    "donde $\\mu$ = mean, $\\sigma$ = standard deviation, y $\\epsilon$ = random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1614870378590,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "L-3qk6ZBm0Fl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Generates a random sample and combines with the encoder output\n",
    "    \n",
    "    Args:\n",
    "      inputs -- output tensor from the encoder\n",
    "\n",
    "    Returns:\n",
    "      `inputs` tensors combined with a random sample\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    mu, sigma = inputs\n",
    "    batch = tf.shape(mu)[0]\n",
    "    dim = tf.shape(mu)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape = (batch, dim))\n",
    "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
    "    ### END CODE HERE ###\n",
    "    return  z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZjCSa7Y-Gvk"
   },
   "source": [
    "### Encoder Layers\n",
    "\n",
    "A continuación, utilice la API funcional para apilar las capas del codificador y generar `mu`, `sigma` y la forma de las características antes de aplanarlas. Esperamos que utilices 3 capas convolucionales, pero siéntase libre de revisarlas como mejor te parezca. Otra sugerencia es usar unidades `1024` en la capa Densa antes de obtener `mu` y `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1614872683027,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "7VSVYjDim4Dk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoder_layers(inputs, latent_dim):\n",
    "  \"\"\"Defines the encoder's layers.\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "\n",
    "  Returns:\n",
    "    mu -- learned mean\n",
    "    sigma -- learned standard deviation\n",
    "    batch_3.shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  x = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 2, \n",
    "                             activation = '', padding = 'same', \n",
    "                             name = 'enc_conv1')(inputs) #TODO Completar capa de activación\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, \n",
    "                             activation = '', padding = 'same', \n",
    "                             name = 'enc_conv2')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3, 3), strides = 2, \n",
    "                             activation = '', padding = 'same', \n",
    "                             name = 'enc_conv3')(x) #TODO Completar capa de activación\n",
    "  batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Flatten(name = 'enc_flatten')(batch_3)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024, activation = '', name = 'enc_dense')(x) #TODO Completar capa de activación\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  mu = tf.keras.layers.Dense(LATENT_DIM, name = 'latent_mu')(x)\n",
    "  sigma = tf.keras.layers.Dense(LATENT_DIM, name = 'latent_sigma')(x)  \n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  # revise `batch_3.shape` here if you opted not to use 3 Conv2D layers\n",
    "  return mu, sigma, batch_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOy7wPPY-g-N"
   },
   "source": [
    "### Encoder Model\n",
    "\n",
    "Alimentará la salida de la función anterior a la `Sampling layer` que definió anteriormente. Eso tendrá las representaciones latentes que se pueden enviar a la red decodificadora más adelante. Complete la siguiente función para construir la red de codificadores con la `Sampling layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1905,
     "status": "ok",
     "timestamp": 1614870962214,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "w8Y-wLFym60N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoder_model(latent_dim, input_shape):\n",
    "  \"\"\"Defines the encoder model with the Sampling layer\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    model -- the encoder model\n",
    "    conv_shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape = input_shape)\n",
    "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim = LATENT_DIM)\n",
    "    z = Sampling()((mu, sigma))\n",
    "    model = tf.keras.Model(inputs, outputs = [mu, sigma, z])\n",
    "    model.summary()\n",
    "    return model, conv_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ENB-6a-0R5"
   },
   "source": [
    "### Decoder Layers\n",
    "\n",
    "A continuación, definirá las capas del decodificador. Esto expandirá las representaciones latentes a las dimensiones de la imagen original. Después de entrenar su modelo VAE, puede usar este modelo de decodificador para generar nuevos datos alimentando entradas aleatorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1614872704419,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "qlTjAzgsm9Vn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, conv_shape):\n",
    "  \"\"\"Defines the decoder layers.\n",
    "  Args:\n",
    "    inputs -- output of the encoder \n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the decoded output\n",
    "  \"\"\"\n",
    "\n",
    "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
    "    x = tf.keras.layers.Dense(units, activation = '', name = 'dec_dense')(inputs) #TODO Completar capa de activación\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]),\n",
    "                              name = 'dec_reshape')(x)\n",
    "  \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (3, 3), \n",
    "                                      strides = 2, activation = '', \n",
    "                                      padding = 'same', name = 'dec_deconv1')(x) #TODO Completar capa de activación\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), \n",
    "                                      strides = 2, activation = '', \n",
    "                                      padding = 'same', name = 'dec_deconv2')(x) #TODO Completar capa de activación\n",
    "    x = tf.keras.layers.BatchNormalization()(x)      \n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 32, kernel_size = (3, 3), \n",
    "                                      strides = 2, activation = '', \n",
    "                                      padding = 'same', name = 'dec_deconv3')(x) #TODO Completar capa de activación\n",
    "    x = tf.keras.layers.BatchNormalization()(x)        \n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = (3, 3), \n",
    "                                      strides = 1, activation = 'sigmoid', \n",
    "                                      padding = 'same', name = 'dec_deconv4')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfLLz84r_MlN"
   },
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1614872707212,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "sUgTyqNFm_jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder_model(latent_dim, conv_shape):\n",
    "  \"\"\"Defines the decoder model.\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    model -- the decoder model\n",
    "  \"\"\"\n",
    "  \n",
    "    inputs = tf.keras.layers.Input(shape = (latent_dim, ))\n",
    "    outputs = decoder_layers(inputs, conv_shape)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.summary()\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps0yuE1d_cQc"
   },
   "source": [
    "### Kullback–Leibler Divergence\n",
    "\n",
    "A continuación, definirás la función para calcular la pérdida de [Kullback-Leibler Divergencia](https://arxiv.org/abs/2002.07514). Esto se utilizará para mejorar la capacidad generativa del modelo. Este código ya está proporcionado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1191,
     "status": "ok",
     "timestamp": 1614872709650,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "tngFmDDwnDn-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
    "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    outputs -- output of the Sampling layer\n",
    "    mu -- mean\n",
    "    sigma -- standard deviation\n",
    "\n",
    "  Returns:\n",
    "    KLD loss\n",
    "  \"\"\"\n",
    "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "  return tf.reduce_mean(kl_loss) * -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi1I431I_og7"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "Define todo el modelo VAE. Recuerda usar `model.add_loss()` para agregar la pérdida de reconstrucción de KL. Se accederá a esto y se agregará a la pérdida más adelante en el ciclo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1614872711528,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "cuPHg28JnGCp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vae_model(encoder, decoder, input_shape):\n",
    "  \"\"\"Defines the VAE model\n",
    "  Args:\n",
    "    encoder -- the encoder model\n",
    "    decoder -- the decoder model\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    the complete VAE model\n",
    "  \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape = input_shape)\n",
    "    mu, sigma, z = encoder(inputs)\n",
    "    reconstructed = decoder(z)\n",
    "\n",
    "    model = tf.keras.Model(inputs, reconstructed)\n",
    "\n",
    "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
    "    model.add_loss(loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_lbWSKbALf-"
   },
   "source": [
    "La siguiente función retornará los modelos `enconder`, `decoder`, y `VAE` que hemos definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1614872714026,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "hnPo0Pr3nIts",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_models(input_shape, latent_dim):\n",
    "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
    "  \n",
    "    encoder, conv_shape = encoder_model(latent_dim = latent_dim, input_shape = input_shape)\n",
    "    decoder = decoder_model(latent_dim = latent_dim, conv_shape = conv_shape)\n",
    "    vae = vae_model(encoder, decoder, input_shape = input_shape)\n",
    "  \n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1614872716309,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "IHdr3CUznL5Z",
    "outputId": "9c26e142-64e4-4628-cf41-0ee84db5cd43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6IwN5vlAb5w"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Ahora configurará el modelo para el entrenamiento. Definimos algunas pérdidas, el optimizador y la métrica de pérdidas a continuación, pero puedes experimentar con otras si lo deseas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1614871980926,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "dHPwSmZFnQ_2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWRzFxYkAvXH"
   },
   "source": [
    "Generarás 16 imágenes en una cuadrícula de 4x4 para mostrar\n",
    "Progreso de la generación de imágenes. Hemos definido una función de utilidad para eso a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1614871984726,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "DGe445j0nTmf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, step, test_input):\n",
    "  \"\"\"Helper function to plot our 16 images\n",
    "\n",
    "  Args:\n",
    "\n",
    "  model -- the decoder model\n",
    "  epoch -- current epoch number during training\n",
    "  step -- current step number during training\n",
    "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
    "  \"\"\"\n",
    "    predictions = model.predict(test_input)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        img = predictions[i, :, :, :] * 255\n",
    "        img = img.astype('int32')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "  # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
    "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgJfazr6A_py"
   },
   "source": [
    "\n",
    "Ahora puedes iniciar el ciclo de entrenamiento. Se le pedirá que seleccione el número de épocas y que complete la subsección sobre la actualización de los pesos. Los pasos generales son:\n",
    "\n",
    "- alimentar un lote de entrenamiento al modelo VAE\n",
    "- calcule la pérdida de reconstrucción (sugerencia: use mse_loss definido anteriormente en lugar de bce_loss en el laboratorio sin calificar, luego multiplique por las dimensiones aplanadas de la imagen (es decir, 64 x 64 x 3)\n",
    "- agregue la pérdida de regularización de KLD a la pérdida total (puede acceder a la propiedad de pérdidas del modelo vae)\n",
    "- obtener los gradientes\n",
    "- use el optimizador para actualizar los pesos\n",
    "\n",
    "Al entrenar a tu `VAE`, es posible que notes que no hay mucha variación en las caras. ¡Pero no dejes que eso te desanime! Las pruebas se basarán en qué tan bien reconstruye las caras originales y no en qué tan bien crea caras nuevas.\n",
    "\n",
    "El entrenamiento también llevará mucho tiempo (más de 30 minutos) y eso es de esperarse. Si utilizó la métrica de pérdida media sugerida anteriormente, entrene el modelo hasta que se reduzca a alrededor de `320` antes de enviarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 5655087,
     "status": "ok",
     "timestamp": 1614878428970,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "hvL1bHXJnajM",
    "outputId": "f136a417-e3c9-4819-8ef5-81e6a21de4be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop. Display generated images each epoch\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
    "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(training_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "       \n",
    "            reconstructed = vae(x_batch_train)\n",
    "      # Compute reconstruction loss\n",
    "            flattened_inputs = tf.reshape(x_batch_train, shape = [-1])\n",
    "            flattened_outputs = tf.reshape(reconstructed, shape = [-1])\n",
    "            loss = mse_loss(flattened_inputs, flattened_outputs) * 64 * 64 * 3\n",
    "            loss += sum(vae.losses)\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    ### END CODE HERE\n",
    "    \n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            display.clear_output(wait=False)    \n",
    "            generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
    "        print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5wfzGfABny6"
   },
   "source": [
    "# Plot Reconstructed Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnQQlWZHaj90"
   },
   "source": [
    "As mentioned, your model will be graded on how well it is able to reconstruct images (not generate new ones). You can get a glimpse of how it is doing with the code block below. It feeds in a batch from the test set and plots a row of input (top) and output (bottom) images. Don't worry if the outputs are a blurry. It will look something like below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1OPMbZOxX9fx8tK6CGVbrMaQdgyOiQJIC\" width=\"75%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 5725,
     "status": "ok",
     "timestamp": 1614880318399,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "TfIbqTIKSXEe",
    "outputId": "d7dccae0-48f3-487d-f046-315fba33d1f9"
   },
   "outputs": [],
   "source": [
    "test_dataset = validation_dataset.take(1)\n",
    "output_samples = []\n",
    "\n",
    "for input_image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "idxs = np.random.choice(64, size=10)\n",
    "\n",
    "vae_predicted = vae.predict(test_dataset)\n",
    "display_results(output_samples[idxs], vae_predicted[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YKUOCA5BtAA"
   },
   "source": [
    "# Plot Generated Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylxL9z15ctsy"
   },
   "source": [
    "Using the default parameters, it can take a long time to train your model well enough to generate good fake anime faces. In case you decide to experiment, we provided the code block below to display an 8x8 gallery of fake data generated from your model. Here is a sample gallery generated after 50 epochs.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1QwElgfg5TY6vCgI1FK6vdI8Bo6UZKfuX\" width=\"75%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "executionInfo": {
     "elapsed": 3644,
     "status": "ok",
     "timestamp": 1614880333374,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "zCpTybvGSS6L",
    "outputId": "e1561b3d-8eb7-4640-b620-989d4bb90ddd"
   },
   "outputs": [],
   "source": [
    "def plot_images(rows, cols, images, title):\n",
    "    '''Displays images in a grid.'''\n",
    "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
    "\n",
    "    plt.figure(figsize=(12,12))       \n",
    "    plt.imshow(grid)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# initialize random inputs\n",
    "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
    "\n",
    "# get predictions from the decoder model\n",
    "predictions= decoder.predict(test_vector_for_generation)\n",
    "\n",
    "# plot the predictions\n",
    "plot_images(8,8,predictions,'Generated Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4IixoasCfoR"
   },
   "source": [
    "### Save the Model\n",
    "\n",
    "Once your satisfied with the results, please save and download the model. Afterwards, please go back to the Coursera submission portal to upload your h5 file to the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1495,
     "status": "ok",
     "timestamp": 1614879315132,
     "user": {
      "displayName": "Ankit Saini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64",
      "userId": "07834131228570077607"
     },
     "user_tz": -330
    },
    "id": "A9E8qwDAVMPs"
   },
   "outputs": [],
   "source": [
    "vae.save(\"anime.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C4W3_Assignment.ipynb",
   "provenance": [
    {
     "file_id": "1vOYc6DuTvxA1E2bTI2DpLt2CAM38K8H3",
     "timestamp": 1614867358563
    }
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
