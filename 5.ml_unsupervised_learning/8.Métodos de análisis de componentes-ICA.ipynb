{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLPJ7WCwfIVO"
   },
   "source": [
    "# **ICA**\n",
    "\n",
    "En ciencia de datos, el análisis de componentes independientes (ICA) es una técnica de reducción de dimensionalidad que se utiliza para transformar un conjunto de datos de muchas variables en un conjunto de datos de pocas variables. El ICA es similar al PCA, pero se centra en la extracción de componentes que son independientes entre sí, en lugar de componentes que capturan la mayor cantidad de varianza posible.\n",
    "\n",
    "El ICA se basa en la idea de que las variables en un conjunto de datos a menudo están correlacionadas entre sí, pero no necesariamente de forma lineal. El ICA encuentra una nueva base de variables, llamada componentes independientes, que son linealmente independientes entre sí. Cada componente independiente es una combinación lineal de las variables originales, y está diseñado para capturar la mayor cantidad de independencia posible entre las variables.\n",
    "\n",
    "El ICA se utiliza en una variedad de aplicaciones de ciencia de datos, incluyendo:\n",
    "\n",
    "- Visualización: El ICA se puede utilizar para visualizar conjuntos de datos de alta dimensionalidad. Al proyectar los datos en un espacio de dos o tres dimensiones, el ICA puede ayudar a los científicos de datos a identificar patrones y tendencias que no serían visibles en el conjunto de datos original.\n",
    "\n",
    "- Aprendizaje automático: El ICA se puede utilizar para mejorar el rendimiento de los modelos de aprendizaje automático. Al reducir la dimensionalidad de los datos, el ICA puede ayudar a los modelos a aprender patrones más fácilmente y a generalizar mejor a nuevos datos.\n",
    "\n",
    "- Preprocesamiento de datos: El ICA se puede utilizar para preparar los datos para su análisis posterior. Al eliminar las variables redundantes o irrelevantes, el ICA puede ayudar a mejorar la precisión y la eficiencia de los algoritmos de análisis de datos.\n",
    "\n",
    "Un ejemplo de cómo se puede utilizar el ICA en ciencia de datos es el análisis de las características de los vinos. Un conjunto de datos de vinos podría incluir variables como el contenido de alcohol, el pH, el contenido de azúcar y el color. El ICA podría utilizarse para reducir estas variables a un número más manejable, como dos o tres componentes independientes. Estos componentes independientes podrían representar la calidad del vino, el tipo de vino o la región de producción.\n",
    "\n",
    "El ICA es una herramienta poderosa que puede ser utilizada en una variedad de aplicaciones de ciencia de datos. Al reducir la dimensionalidad de los datos, el ICA puede ayudar a los científicos de datos a identificar patrones y tendencias, mejorar el rendimiento de los modelos de aprendizaje automático y preparar los datos para su análisis posterior.\n",
    "\n",
    "Diferencias entre PCA e ICA\n",
    "\n",
    "- El PCA y el ICA son dos técnicas de reducción de dimensionalidad, pero tienen algunas diferencias clave. El PCA se centra en la extracción de componentes que capturan la mayor cantidad de varianza posible en el conjunto de datos. El ICA se centra en la extracción de componentes que son independientes entre sí.\n",
    "\n",
    "- En términos prácticos, esto significa que el PCA puede ser mejor para la visualización de datos, mientras que el ICA puede ser mejor para el aprendizaje automático. El PCA también es generalmente más fácil de implementar que el ICA.\n",
    "\n",
    "## Ventajas del ICA\n",
    "\n",
    "El ICA tiene algunas ventajas sobre el PCA, incluyendo:\n",
    "\n",
    "- Capacidad para extraer componentes independientes: El ICA es capaz de extraer componentes que son independientes entre sí, lo que puede ser útil para el aprendizaje automático y el preprocesamiento de datos.\n",
    "\n",
    "- Resistencia a los datos ruidosos: El ICA es más resistente a los datos ruidosos que el PCA.\n",
    "\n",
    "- Mayor flexibilidad: El ICA es más flexible que el PCA, ya que no requiere que las variables estén correlacionadas linealmente.\n",
    "\n",
    "## Desventajas del ICA\n",
    "\n",
    "El ICA también tiene algunas desventajas, incluyendo:\n",
    "\n",
    "- Dificultad de implementación: El ICA puede ser más difícil de implementar que el PCA.\n",
    "\n",
    "- Bajo rendimiento en la visualización: El ICA puede no ser tan bueno como el PCA para la visualización de datos.\n",
    "\n",
    "- En general, el ICA es una técnica de reducción de dimensionalidad poderosa que puede ser utilizada en una variedad de aplicaciones de ciencia de datos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLPeYHAPMrp4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import FastICA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = [12, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzZ4fvx_L8FW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 10, n_samples)\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "S = np.c_[s1, s2]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0.5, 2]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "# Compute ICA\n",
    "ica = FastICA()\n",
    "S_ = ica.fit(X).transform(X)  # Get the estimated sources\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "#assert np.allclose(X, np.dot(S_, A_.T))\n",
    "\n",
    "###############################################################################\n",
    "# Plot results\n",
    "pl.figure()\n",
    "pl.subplot(3, 1, 1)\n",
    "pl.plot(S)\n",
    "pl.title('True Sources')\n",
    "pl.subplot(3, 1, 2)\n",
    "pl.plot(X)\n",
    "pl.title('Observations (mixed signal)')\n",
    "pl.subplot(3, 1, 3)\n",
    "pl.plot(S_)\n",
    "pl.title('ICA estimated sources')\n",
    "pl.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpg08nF8Mjbg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "# Mix data\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "# We can `prove` that the ICA model applies by reverting the unmixing.\n",
    "assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot results\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "models = [X, S, S_]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(3, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
