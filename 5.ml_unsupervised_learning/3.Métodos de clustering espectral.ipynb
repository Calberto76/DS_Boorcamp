{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1iE9XTGLU2V"
   },
   "source": [
    "# Métodos de clustering espectral\n",
    "\n",
    "Los algoritmos de agrupamiento tradicionales, tales como K-means o las mixturas gaussianas, consideran por definición la existencia de clústeres con forma convexa. Es decir, se trata de conjuntos esféricos o elípticos agrupados en torno a un centro o centroide. Las técnicas de agrupamiento espectral (spectral clustering en inglés), en cambio, transforman el conjunto de datos de entrenamiento y lo representan en espacios alternativos donde se simplifica la identificación de los distintos clústeres sea cual sea su forma. Así, este tipo de técnicas novedosas permite identificar agrupamientos con formas y densidades variadas. Es necesario darse cuenta de que la identificación de los agrupamientos en estos conjuntos de datos de ejemplos mediante técnicas de agrupamiento tradicionales sería difícil o imposible\n",
    "\n",
    "Así pues, el objetivo de un algoritmo de agrupamiento espectral podría replantearse como la búsqueda de un grupo de subconjuntos de nodos (que, en nuestro caso, representan ejemplos del conjunto de entrena- miento) tal que la probabilidad de transitar entre ellos sea mínima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpRUPzfm-ZM8"
   },
   "source": [
    "En esta práctica vamos a ver cómo funciona el clustering espectral.\n",
    "\n",
    "Para ello, como siempre, lo primero es cargar las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPsbbl_TR6cw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "float_formatter = lambda x: \"%.3f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3ArN8e4RIrN"
   },
   "source": [
    "En nuestro primer ejemplo vamos a usar un dataset muy sencillo que nos permita entender lo que hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JczwWrBNRIYY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 3], [2, 1], [1, 1],\n",
    "    [3, 2], [7, 8], [9, 8],\n",
    "    [9, 9], [8, 7], [13, 14],\n",
    "    [14, 14], [15, 16], [14, 15]\n",
    "])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1])\n",
    "for i, txt in enumerate(range(X.shape[0])):\n",
    "    ax.annotate(txt, X[i])\n",
    "plt.xlabel('Ancho')\n",
    "plt.ylabel('Largo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8YD9ONVSWrG"
   },
   "source": [
    "Tal y como acabamos de ver, el agrupamiento espectral consiste en 4 pasos básicos:\n",
    "\n",
    "-   La obtención de la matriz de adyacencias o afinidad,\n",
    "-   La obtención de la matriz Laplaciana,\n",
    "-   El cálculo de los vectores y valores propios de esta última,\n",
    "-   El clustering mediante K-means (u otra técnica tradicional).\n",
    "\n",
    "Vamos a por la primera, la creación de la matriz de adyacencias o afinidad:\n",
    "\n",
    "La forma más común de matriz en el análisis de redes sociales es una matriz cuadrada muy simple con tantas filas y columnas como actores haya en nuestro conjunto de datos. Los “elementos” o puntuaciones en las celdas de la matriz registran información sobre los vínculos entre cada par de actores.\n",
    "\n",
    "La matriz más simple y común es la binaria. Es decir, si está presente un empate, se ingresa uno en una celda; si no hay empate, se ingresa un cero. Este tipo de matriz es el punto de partida para casi todos los análisis de redes, y se llama “matriz de adyacencia” porque representa quién está al lado, o adyacente a quién en el “espacio social” mapeado por las relaciones que hemos medido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-A_u19doRIVj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_dist = pairwise_distances(X,metric='euclidean')\n",
    "print(W_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-RmbITJ29hQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXKOs8MQbeZc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(W_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai-lFHmHTpeh"
   },
   "source": [
    "Vamos a utilizar el paquete networkx para visualizar el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construímos la matriz de adyacencias o afinidad\n",
    "W_ad = np.zeros_like(W_dist, np.uint8)\n",
    "W_ad[W_dist < 5 ] = 1\n",
    "print(W_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Aeg2hJpRIQv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_graph(G):\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf7Ya4W6RIN4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(W_ad)\n",
    "draw_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "137TFJ-BUnPO"
   },
   "source": [
    "`networkx` también nos permite calcular la matriz de afinidad a partir del grafo. Vamos a ver como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yprms7ohVqXQ"
   },
   "source": [
    "Ahora tenemos que obtener la matriz Laplaciana, que recordad que se obtenía como la resta entre la matriz de adyacencia y la matriz de grado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_ad = nx.adjacency_matrix(G)\n",
    "print(W_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos devuelve algo que no parece una matriz. Pues esto que acabamos de ver es una matriz `sparse`, que simplemente es una matriz con muy pocos elementos diferentes de 0.\n",
    "\n",
    "Para verla en formato matriz necesitamos hacer uso del método `todense()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPFVfUERRIDa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matriz de grado\n",
    "import numpy as np\n",
    "D = np.diag(np.sum(np.array(W_ad.todense()),axis = 0)) # Le ponemos axis 1 para que nos sume por reglón\n",
    "print('Matriz de grado:\\n', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwztnBdORH5h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matriz laplaciana\n",
    "L = D - W_ad\n",
    "print('Matriz Laplaciana:\\n', L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OL793P0WH1b"
   },
   "source": [
    "La matriz Laplaciana es una matriz especial con ciertas propiedades que nos facilitan la vida. Una de ellas es que:\n",
    "\n",
    "-   **Si el grafo `W` tiene `K` componentes conexas, entonces `L` tiene `K` vectores propios (*eigenvectors*) con valor propio (*eigenvalue*) igual a 0.**\n",
    "\n",
    "En este caso, cuantos eigenvectors vamos a tener iguales a 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--qzZ4eXWHru",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eigenvalues / eigenvectors\n",
    "e, v = eig(L)\n",
    "# eigenvalues\n",
    "print('eigenvalues:')\n",
    "print(e)\n",
    "# eigenvectors\n",
    "print('eigenvectors:')\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmWEZ2eeWHoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(e)\n",
    "plt.title('eigenvalues')\n",
    "\n",
    "eigen_0 = np.where(e < 10e-6)[0]\n",
    "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
    "for n, i in enumerate(eigen_0):\n",
    "    plt.figure()\n",
    "    plt.plot(v[:, i])\n",
    "    plt.title(f'{n} eigenvector con eigenvalue=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du5LkK6xWHl4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos nuestro dataset de nuevo\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')\n",
    "for i, txt in enumerate(range(X.shape[0])):\n",
    "    ax.annotate(txt, X[i])\n",
    "plt.xlabel('Ancho')\n",
    "plt.ylabel('Largo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbkqy5bRZv56"
   },
   "source": [
    "Si nos fijamos en las gráficas el vector propio de la matriz L (cuyos valores propios son 0), podemos ver cómo nos permiten separar correctamente los 3 clusters de nuestro dataset.\n",
    "\n",
    "Recordad que esto es buena señal, nos indica que con esta nueva representación de los datos, un algoritmo de clustering podrá separarlos correctamente.\n",
    "\n",
    "Vamos a comprobar si es cierto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ttw18fScfl_N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigen_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpNusooYWHjQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectores_propios_a_utilizar = eigen_0 # en este caso queremos usar los 3, podría ser diferente\n",
    "\n",
    "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
    "km = KMeans(init='k-means++', n_clusters=len(eigen_0))\n",
    "km.fit(Xnew)\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--tevhxAWHgZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el resultado del clustering\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=km.labels_)\n",
    "for i, txt in enumerate(range(X.shape[0])):\n",
    "    ax.annotate(txt, X[i])\n",
    "plt.xlabel('Ancho')\n",
    "plt.ylabel('Largo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tgghAcvbdZP"
   },
   "source": [
    "¿Y si visualizamos los datos proyectados usando los 3 vectores propios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69g9goaJWHd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el dataset proyectado usando los 3 vectores propios escogidos\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Xnew[:, 0], Xnew[:, 1], c=km.labels_)\n",
    "plt.xlabel('Ancho')\n",
    "plt.ylabel('Largo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhZqn_-Sb1qC"
   },
   "source": [
    "Ahora para k-means es facil trabajar con esta nueva representación de los datos. \n",
    "\n",
    "Como podes comprobar, al final, lo único que estamos haciendo es cambiar el espacio de representación de los datos por uno que nos permita agruparlos mejor.\n",
    "\n",
    "Es básicamente el modo de funcionamiento de las redes neuronales profundas, en las que los datos van sufriendo transformaciones en cada capa hasta llegar a una representación que facilita la tarea a desarrollar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2\n",
    "\n",
    "Vamos a complicarlo un poco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SAseW2wWHar",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, clusters = make_circles(n_samples=1000, noise=.05, factor=.5, random_state=0)\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0N9tu6cf53"
   },
   "source": [
    "Vamos a probar con el k-means, a ver qué tal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4onVZLqWHQn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2)\n",
    "km_clustering = km.fit(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=km_clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGcwcPihctWE"
   },
   "source": [
    "K-means no es capaz de hacer el agrupamiento de forma correcta.\n",
    "\n",
    "Vamos a ver si con lo que acabamos de aprender somos capaces. Para ello, recorda que necesitamos:\n",
    "\n",
    "- La obtención de la matriz de adyacencias o afinidad   \n",
    "- La obtención de la matriz Laplaciana\n",
    "- El cálculo de los vectores y valores propios de esta última\n",
    "- El clustering mediante K-means (u otra técnica tradicional)\n",
    "\n",
    "Manos a la obra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXS0LMnTddgz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_dist = pairwise_distances(X, metric = 'euclidean')\n",
    "print('Matriz distancias:\\n', W_dist)\n",
    "plt.imshow(W_dist)\n",
    "plt.colorbar()\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtUeTehYddeM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vamos a escoger 0.5 como el umbral\n",
    "W_ad = np.zeros_like(W_dist, np.uint8)\n",
    "W_ad[W_dist < 0.5] = 1\n",
    "print('Matriz de adyacencia:\\n', W_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1LQsPqfddbz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matriz de grado\n",
    "D = np.diag(np.sum(W_ad,axis = 0))\n",
    "print('Matriz de grado:\\n', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH3ZACRBddZD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matriz laplaciana\n",
    "L = D - W_ad\n",
    "print('Matriz Laplaciana:\\n', L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLIXJLBrddWj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculamos los eigenvectors y eigenvalues\n",
    "e, v = np.linalg.eig(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GWH67f5ddT0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(e)\n",
    "plt.title('eigenvalues')\n",
    "\n",
    "eigen_0 = np.isclose(e, 0, atol=1e-3)\n",
    "print(f'Tenemos {sum(eigen_0)} componentes conexos en nuestro dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh9cOJS7iZlA"
   },
   "source": [
    "Mirando los eigenvalues parece no que sea fácil elegir con qué componentes queremos quedarnos para hacer el clustering, ¿verdad?\n",
    "\n",
    "¿Probamos a calcular la matriz laplaciana normalizada simétrica a ver si mejora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZVv6l0fghTt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculamos la laplaciana normalizada simétrica\n",
    "D = np.sum(W_ad, axis=1)\n",
    "D = D**(-1./2)\n",
    "I = np.diag(np.ones(D.size))\n",
    "D = np.diag(D)\n",
    "L = I - D.dot(W_ad)\n",
    "\n",
    "\n",
    "# calculamos autovectores y autovalores\n",
    "e, v = np.linalg.eig(L)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(e)\n",
    "plt.title('eigenvalues')\n",
    "\n",
    "eigen_0 = np.where(np.isclose(e, 0, atol=1e-1))[0]\n",
    "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
    "for n, i in enumerate(eigen_0):\n",
    "    plt.figure()\n",
    "    plt.plot(v[:, i])\n",
    "    plt.title(f'{n} eigenvector con eigenvalue=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsdER_bliumq"
   },
   "source": [
    "En este caso parece que encuentra las 6 componentes conexas! Vamos a ver si funciona el clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1648755826137,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "userId": "08557990006797564533"
     },
     "user_tz": -120
    },
    "id": "GamcL2_WhE0y",
    "outputId": "759b684e-fbb8-40dd-efc0-fda57795a0af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectores_propios_a_utilizar = eigen_0\n",
    "print(vectores_propios_a_utilizar)\n",
    "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
    "km = KMeans(init='k-means++', n_clusters=2)\n",
    "km.fit(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTi9MgX4h_5S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el resultado del clustering\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axQTgWsPi64o"
   },
   "source": [
    "Parece que no. Vamos a ver los datos en su nuevo espacio de representación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sOiT5N6i4s3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el dataset proyectado usando los 3 vectores propios escogidos\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Xnew[:,0], Xnew[:,1], c=km.labels_, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRkCjlYkgDhi"
   },
   "source": [
    "Mirando la nueva representación de los datos, no parece que el k-means vaya a poder hacer bien el clustering, ¿verdad?\n",
    "\n",
    "¿Se te ocurre alguna forma mejor de seguir?\n",
    "\n",
    "¿Quizá pensando en cómo hemos construído la matriz de adyacencia? ¿Existe alguna otra forma de hacerlo?\n",
    "\n",
    "Eso es, podemos intentarlo haciendo uso del kNN en vez del umbral ($\\epsilon$).\n",
    "\n",
    "Vamos allá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h-Ghy01ddQz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matriz_afinidad_KNN(mSimilitud, KNN=5):\n",
    "    auxM = mSimilitud.copy()\n",
    "    np.fill_diagonal(auxM, 0)\n",
    "\n",
    "    # Construimos la matriz afinidad de A a B\n",
    "    mAfinidadA = np.zeros(auxM.shape)\n",
    "    # utilizamos la matriz similitud para ordenar los vecinos de cada nodo por cercanía\n",
    "    # `np.argsort` nos devuelve los índices que ordenan el array introducido (de forma ascendente)\n",
    "    # en este caso, al tener la matriz similitud, tenemos que hacerla negativa, para que nos \n",
    "    # devuelva primero los más similares (que al ponerle el - a la matriz de similitud, son los\n",
    "    # valores más pequeños)\n",
    "    # Flatten simplemente lo utilizamos para convertir la matriz `KNN x n`en un vector de 1x(KNN x n)\n",
    "    indices_kNN_nodo_fila = np.argsort(-auxM, axis=0)[0:KNN, :].flatten()\n",
    "    # Nos creamos un array para combinar con el anterior y poder indexar los \n",
    "    # elementos pertinentes de la matriz de afinidad\n",
    "    indices_kNN_nodo_col = np.tile(np.arange(auxM.shape[0]), KNN)\n",
    "    mAfinidadA[indices_kNN_nodo_fila, indices_kNN_nodo_col] = 1\n",
    "    np.fill_diagonal(mAfinidadA, 1)\n",
    "\n",
    "    # Y ahora hacemos lo mismo de B a A\n",
    "    mAfinidadB = np.zeros(auxM.shape)\n",
    "    # Fijaos que en este caso la matriz va a ser `n x KNN` (hacemos el argsort en la dirección columnas: -->)\n",
    "    indices_kNN_nodo_fila = np.repeat(np.arange(auxM.shape[0]), KNN)\n",
    "    indices_kNN_nodo_col = np.argsort(-auxM, axis=1)[:, 0:KNN].flatten()\n",
    "    mAfinidadB[indices_kNN_nodo_fila, indices_kNN_nodo_col] = 1\n",
    "    np.fill_diagonal(mAfinidadB, 1)\n",
    "\n",
    "    return (mAfinidadA + mAfinidadB) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1648755902632,
     "user": {
      "displayName": "Félix José Fuentes Hurtado",
      "userId": "08557990006797564533"
     },
     "user_tz": -120
    },
    "id": "35bSTa4VNXx2",
    "outputId": "410c77b5-aef2-433e-f441-27fa802287a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculamos de nuevo la matriz de adyacencia (o afinidad), esta vez siguiendo el enfoque de los kNN\n",
    "sigma = 0.1\n",
    "W_sim = np.exp(-np.power(W_dist,2)/(2*sigma**2))\n",
    "W_ad = matriz_afinidad_KNN(W_sim)\n",
    "print(W_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRDBlsZYddOM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculamos la laplaciana normalizada simétrica\n",
    "D = np.sum(W_ad,axis=1)\n",
    "D = D**(-1./2)\n",
    "I = np.diag(np.ones(D.size))\n",
    "D = np.diag(D)\n",
    "L = I - D.dot(W_ad).dot(D)\n",
    "\n",
    "# calculamos autovectores y autovalores\n",
    "e, v = np.linalg.eig(L)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(e)\n",
    "plt.title('eigenvalues')\n",
    "\n",
    "eigen_0 = np.where(np.isclose(e, 0, atol=1e-5))[0]\n",
    "print(f'Tenemos {len(eigen_0)} componentes conexos en nuestro dataset.')\n",
    "for n, i in enumerate(eigen_0):\n",
    "    plt.figure()\n",
    "    plt.plot(v[:, i])\n",
    "    plt.title(f'{n} eigenvector con eigenvalue=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3hEMqIXkNbr"
   },
   "source": [
    "Parece que ahora encuentra 2 componentes conexos! Vamos a probar a hacer el clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnR5__w2ddLi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectores_propios_a_utilizar = eigen_0\n",
    "print(vectores_propios_a_utilizar)\n",
    "Xnew = np.array(v[:, vectores_propios_a_utilizar])\n",
    "km = KMeans(init='k-means++', n_clusters=2)\n",
    "km.fit(Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxWozte5dYVx"
   },
   "source": [
    "Veamos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FlHni66ki5l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el resultado del clustering\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=km.labels_, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpQVga8HdHDn"
   },
   "source": [
    "Es muy importante elegir el tipo de enfoque adecuado dependiendo de los datos. Cuando hemos construido la $W_{ad}$ mediante el método del umbral, nos hemos inventado completamente dicho umbral.\n",
    "\n",
    "Esto no quiere decir que no exista un umbral para el cual no funcione correctamente el clustering siguiendo ese enfoque, lo que quiere decir es que si no conocemos el umbral, es posiblemente más sencillo seguir el enfoque de los kNN.\n",
    "\n",
    "Vamos a ver los datos en su nuevo espacio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3oM8GERlDbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# veamos el dataset proyectado usando los 3 vectores propios escogidos\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Xnew[:,0], Xnew[:,1], c=km.labels_, alpha=0.7, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo3qomiulFm_"
   },
   "source": [
    "Maravilloso, ¿no parece?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Más maravilloso va a parecer aún lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMZTNn5_cs95",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=0)\n",
    "sc_clustering = sc.fit(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=sc_clustering.labels_, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9beWrCFQcspj"
   },
   "source": [
    "En efecto, así de rápido podemos realizar un clustering espectral utilizando las librerías existentes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BErmLZr8ghjU"
   },
   "source": [
    "Fuente: https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045\n",
    "\n",
    "Ejemplo paso a paso para entender y profundizar:\n",
    "-  https://towardsdatascience.com/spectral-clustering-aba2640c0d5b\n",
    "\n",
    "Más ejemplos:\n",
    "\n",
    "-  https://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html\n",
    "-  https://scikit-learn.org/stable/auto_examples/cluster/plot_coin_segmentation.html\n",
    "\n",
    "Lecturas discutiendo la similitud de k-means, clustering espectral y PCA, para quien tenga interés:\n",
    "\n",
    "-  https://stats.stackexchange.com/a/151665\n",
    "-  https://stats.stackexchange.com/a/189324\n",
    "-  https://stats.stackexchange.com/a/189324 (muy completa y didáctica)\n",
    "-  https://qr.ae/TW25h8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación entre Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from itertools import cycle, islice\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n",
    ")\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"allow_single_cluster\": True,\n",
    "    \"hdbscan_min_cluster_size\": 15,\n",
    "    \"hdbscan_min_samples\": 3,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.08,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        noisy_moons,\n",
    "        {\n",
    "            \"damping\": 0.75,\n",
    "            \"preference\": -220,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.01,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n",
    "    (no_structure, {}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    # ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        n_init=\"auto\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "    )\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        metric=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        connectivity=connectivity,\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"MiniBatch\\nKMeans\", two_means),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"Spectral\\nClustering\", spectral)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                + \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
